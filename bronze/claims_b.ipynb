{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f63dbb5b-8b54-4b12-ae26-819f78244acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# service principal for integrating with ADLS and access it's data\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.hpadlsacc.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.hpadlsacc.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.hpadlsacc.dfs.core.windows.net\", dbutils.secrets.get(\"hc-secret-scope\", \"app-key\"))\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.hpadlsacc.dfs.core.windows.net\", dbutils.secrets.get(\"hc-secret-scope\", \"service-cred\"))\n",
    "tenant_id = dbutils.secrets.get(\"hc-secret-scope\", \"dir-id\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.hpadlsacc.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ffb794e-b4b5-4930-bfba-abffbfa6ff6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loading the data to bronze layer from Landing container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24b4b029-3c98-4e0d-9aa6-bea9004ae828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing the required spark session and functions\n",
    "from pyspark.sql import SparkSession, functions as f\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "src_path = \"abfss://landing@hpadlsacc.dfs.core.windows.net/claims/*.csv\"\n",
    "\n",
    "# reading all claim files\n",
    "claims_df = spark.read.format('csv').option('header',True).option(\"includeMetadata\", \"true\").load(src_path) \n",
    "\n",
    "# adding datasource column based on the file name\n",
    "claims_df = claims_df.withColumn(\n",
    "    \"datasource\",\n",
    "    when(col(\"_metadata.file_path\").contains(\"hospital1\"), \"hosa\")\n",
    "    .when(col(\"_metadata.file_path\").contains(\"hospital2\"), \"hosb\")\n",
    "    .otherwise(None)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b891be8-de12-42d0-857a-1733c53d5cbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# selecting a set of records for clarification\n",
    "claims_df.filter(col('datasource')=='hosa').display(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90927a11-967e-4f43-837b-0cbf6fc6248c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# defining the destination path\n",
    "target_path = \"abfss://bronze@hpadlsacc.dfs.core.windows.net/claims/\"\n",
    "\n",
    "# finally writing the dataframe to the target path\n",
    "claims_df.write.format(\"parquet\").mode(\"overwrite\").save(target_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "claims_b",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
